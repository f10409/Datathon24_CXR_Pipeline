{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ba7dbd-772c-429d-ad80-b4547ff99c3c",
   "metadata": {},
   "source": [
    "# ⚠️⚠️⚠️⚠️⚠️ ***Attention*** ⚠️⚠️⚠️⚠️⚠️\n",
    "\n",
    "Please specify your assigned GPU number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca0332-c712-4e3a-af99-e207ff7a41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = # put GPU number here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166b173-197b-4c00-bdfb-69554e9fb565",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Model Evaluation\n",
    "\n",
    "##### This notebook demonstrates the evaluation of our trained model across two distinct datasets. We'll assess its performance using\n",
    "\n",
    "##### 1. Internal test data: This is derived from the same source as our training data (**Data1**).\n",
    "##### 2. External test data: We'll use an independent dataset (**Data3**) to gauge the model's generalization capabilities.\n",
    "\n",
    "##### By comparing results from both internal and external tests, we can gain insights into the model's robustness and its ability to handle diverse data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d6352-dd4f-4318-83b2-61c1ec0daa21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1. Import essential libraries and evironment setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef43504-a2ed-433e-9168-ee69fc95c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b79bdc-6450-43c0-946c-dfe6241b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37feeb7a-48d5-486c-bba9-11e6c4e5af7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2. Load the best model from 'Model_training' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e5c1f-545b-4f76-b10f-455f1389a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./weights/best_model_Data1.keras')\n",
    "\n",
    "# Load the following model trained by us if you haven't had your own model yet.\n",
    "# model = load_model('/fsx/embed/summer-school-24/Datathon24_SummerSchool_CXR/weights/best_model_Data1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e132f-9707-4425-8468-a949f9eb3ad2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.3. Load and process testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7d11f-fc19-4077-8823-8bb815d4d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for image preprocessing \n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array / 255.0\n",
    "\n",
    "# Load and preprocess test images\n",
    "\n",
    "positive_paths = glob('/fsx/embed/summer-school-24/Datathon24_SummerSchool_CXR/Data1_Preprocessed/test/positive/*.png')\n",
    "negative_paths = glob('/fsx/embed/summer-school-24/Datathon24_SummerSchool_CXR/Data1_Preprocessed/test/negative/*.png')\n",
    "\n",
    "#positive_paths = glob('/home/fli40/Data/Datathon24_SummerSchool_CXR/Data1_Preprocessed/test/positive/*.png')\n",
    "#negative_paths = glob('/home/fli40/Data/Datathon24_SummerSchool_CXR/Data1_Preprocessed/test/negative/*.png')\n",
    "\n",
    "test_image_paths = positive_paths + negative_paths  # Add all test image paths\n",
    "test_labels = [1]*len(positive_paths) + [0]*len(negative_paths)  # Add corresponding true labels (0 or 1 for binary classification)\n",
    "\n",
    "X_test = np.vstack([load_and_preprocess_image(img_path) for img_path in test_image_paths])\n",
    "y_true = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170e0130-ccde-44ca-9e88-59fb5d816394",
   "metadata": {},
   "source": [
    "### 2.3.1. Show some testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7a55b-c496-4dfd-8d5b-45e7bac7b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(min(len(X_test), 9)):  # Display up to 9 images\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow((X_test[i*50]*255).astype(\"uint8\"))\n",
    "    plt.title(f\"Label: {y_true[i*50]}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e2bc8-c437-46dc-bc9e-5a89ea9a884e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.4. Make predictions\n",
    "The model is predicting whether each instance belongs to one of two classes (e.g., \"yes\" or \"no\", \"positive\" or \"negative\"). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad012f4-2091-44c7-8003-c2b2f0028062",
   "metadata": {},
   "source": [
    "### 3.4.1. Output probability\n",
    "\n",
    "The output, y_pred_prob, contains predicted probabilities that each instance in X_test belongs to the positive class (class 1).\n",
    "##### 💡 Example: If the model predicts a 0.7 probability for an instance, it suggests there's a 70% chance that the instance belongs to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930f040-cbcb-4d64-aece-e91ed660dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd799a2e-8154-4228-b9ff-07393b5af415",
   "metadata": {},
   "source": [
    "### 3.4.2. Output binary label\n",
    "This line converts the predicted probabilities into binary class predictions (0 or 1).\n",
    "##### 💡 Example: If y_pred_prob for an instance is 0.7, y_pred for that instance will be 1 because 0.7 > 0.5. If y_pred_prob is 0.3, y_pred will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b7184-934c-4348-bae6-4ff8aef1d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Assuming 0.5 as threshold for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338f750-ef8b-4e8e-8c26-9f9ab50318dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results as csv\n",
    "df_results = pd.DataFrame(test_image_paths, columns=['Path'])\n",
    "df_results['Label'] = y_true\n",
    "df_results['Prediction'] = y_pred\n",
    "df_results['Probability'] = y_pred_prob\n",
    "df_results.to_csv('./results/Model1_TestSet.csv', index=False)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873bd9e7-2160-41db-89c6-8a194f1dd8e2",
   "metadata": {},
   "source": [
    "### 3.4.3. Show some testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf395b-ad16-4930-9bde-b8da86eb8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(min(len(X_test), 9)):  # Display up to 9 images\n",
    "    prediction = model.predict(X_test[i*50:i*50+1,:,:,:])[0][0]\n",
    "    predicted_class = 1 if prediction > 0.5 else 0\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow((X_test[i*50]*255).astype(\"uint8\"))\n",
    "    plt.title(f\"Label: {y_true[i*50]} \\n Predicted class: {predicted_class} \\n Confidence: {prediction:.4f}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91bb7e-37d3-4f7c-b354-490290fefc64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.5. Evaluate model with metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda30ef-5fcd-4b41-9922-31cabdc23ce2",
   "metadata": {},
   "source": [
    "### 3.5.1. Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670f165-ca5c-475a-a5d2-c83fe2ea09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues',fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce1dfb-bab2-439d-8883-2a70365cdc29",
   "metadata": {},
   "source": [
    "### 3.5.2. Model Evaluation Metrics\n",
    "\n",
    "To comprehensively assess our model's performance, we employ a diverse set of evaluation metrics:\n",
    "\n",
    "#### 1. Accuracy\n",
    "- The proportion of correct predictions (both true positives and true negatives) among the total number of cases examined.\n",
    "- **Formula:** $Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "  \n",
    "  Where:\n",
    "  - TP = True Positives\n",
    "  - TN = True Negatives\n",
    "  - FP = False Positives\n",
    "  - FN = False Negatives\n",
    "\n",
    "#### 2. Sensitivity, also known as Recall\n",
    "- The proportion of actual positive cases that were correctly identified.\n",
    "- **Formula:** $Sensitivity = \\frac{TP}{TP + FN}$\n",
    "\n",
    "#### 3. Specificity\n",
    "- The proportion of actual negative cases that were correctly identified.\n",
    "- **Formula:** $Specificity = \\frac{TN}{TN + FP}$\n",
    "\n",
    "#### 4. PPV (Positive Predictive Value), also known as Precision\n",
    "- The proportion of positive identifications that are actually correct.\n",
    "- **Formula:** $PPV = \\frac{TP}{TP + FP}$\n",
    "\n",
    "#### 5. F1 Score\n",
    "- The harmonic mean of precision and recall, providing a balanced measure of the model's accuracy.\n",
    "- **Formula:** $F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\n",
    "\n",
    "#### 6. AUC (Area Under the ROC Curve)\n",
    "- Measures the model's ability to distinguish between classes across all classification thresholds.\n",
    "- AUC is calculated by plotting the True Positive Rate against the False Positive Rate at various threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a04fa1-5f5d-42d6-96f9-deed5d24c56c",
   "metadata": {},
   "source": [
    "<img src=\"files/class_score.png\" alt=\"Drawing\" style=\"height: 400px;\"/>\n",
    "\n",
    "Ref: https://permetrics.readthedocs.io/en/latest/pages/classification/F1S.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec6cbd-e0d6-47c3-b5b5-bf9a2f4d1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "auc_1 = roc_auc_score(y_true, y_pred_prob)\n",
    "f1_1 = f1_score(y_true, y_pred)\n",
    "accuracy_1 = accuracy_score(y_true, y_pred)\n",
    "ppv_1 = precision_score(y_true, y_pred)  # PPV is the same as precision\n",
    "sensitivity_1 = recall_score(y_true, y_pred)  # Sensitivity is the same as recall\n",
    "specificity_1 = recall_score(y_true, y_pred, pos_label=0)  # Specificity is recall of the negative class\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC: {auc_1:.4f}\")\n",
    "print(f\"F1 Score: {f1_1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_1:.4f}\")\n",
    "print(f\"PPV (Precision): {ppv_1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity_1:.4f}\")\n",
    "print(f\"Specificity: {specificity_1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f73036-5851-44f9-a686-53d95c0ee236",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob) \n",
    "# Plot the ROC curve\n",
    "plt.figure()  \n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee87c3-7634-42b1-bb3e-d20c9486fe91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.6. External Validation\n",
    "We then use Data3 as external dataset for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8d625-58d7-4bed-9f58-16e84e3e40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess test images\n",
    "\n",
    "positive_paths = glob('/fsx/embed/summer-school-24/Datathon24_SummerSchool_CXR/Data3_Preprocessed/positive/*.png')\n",
    "negative_paths = glob('/fsx/embed/summer-school-24/Datathon24_SummerSchool_CXR/Data3_Preprocessed/negative/*.png')\n",
    "\n",
    "#positive_paths = glob('/home/fli40/Data/Datathon24_SummerSchool_CXR/Data3_Preprocessed/positive/*.png')\n",
    "#negative_paths = glob('/home/fli40/Data/Datathon24_SummerSchool_CXR/Data3_Preprocessed/negative/*.png')\n",
    "\n",
    "test_image_paths = positive_paths + negative_paths  # Add all test image paths\n",
    "test_labels = [1]*len(positive_paths) + [0]*len(negative_paths)  # Add corresponding true labels (0 or 1 for binary classification)\n",
    "\n",
    "X_test = np.vstack([load_and_preprocess_image(img_path) for img_path in test_image_paths])\n",
    "y_true = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15666f8c-a08b-4f44-a90e-7304b648de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # Assuming 0.5 as threshold for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b382996e-2bd3-4900-a8b7-965e095c41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results as csv\n",
    "df_results = pd.DataFrame(test_image_paths, columns=['Path'])\n",
    "df_results['Label'] = y_true\n",
    "df_results['Prediction'] = y_pred\n",
    "df_results['Probability'] = y_pred_prob\n",
    "df_results.to_csv('./results/Model1_External.csv', index=False)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b3146-5813-4d95-816d-05524e4e8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues',fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e60e8-fe9e-4e61-a617-c4bccbf43bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "auc_3 = roc_auc_score(y_true, y_pred_prob)\n",
    "f1_3 = f1_score(y_true, y_pred)\n",
    "accuracy_3 = accuracy_score(y_true, y_pred)\n",
    "ppv_3 = precision_score(y_true, y_pred)  # PPV is the same as precision\n",
    "sensitivity_3 = recall_score(y_true, y_pred)  # Sensitivity is the same as recall\n",
    "specificity_3 = recall_score(y_true, y_pred, pos_label=0)  # Specificity is recall of the negative class\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC: {auc_3:.4f}\")\n",
    "print(f\"F1 Score: {f1_3:.4f}\")\n",
    "print(f\"Accuracy: {accuracy_3:.4f}\")\n",
    "print(f\"PPV (Precision): {ppv_3:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity_3:.4f}\")\n",
    "print(f\"Specificity: {specificity_3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1790c-ad46-4410-bd30-039c6cfb3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob) \n",
    "# Plot the ROC curve\n",
    "plt.figure()  \n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1587e-635d-4550-8d47-09a45f99408b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.7. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97392479-b22d-4e93-9536-90ae229df405",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[\"AUC\",auc_1, auc_3], [\"F1 Score\",f1_1, f1_3], [\"Accuracy\",accuracy_1, accuracy_3], [\"PPV\",ppv_1, ppv_3], [\"Sensitivity\",sensitivity_1, sensitivity_3], [\"Specificity\",specificity_1, specificity_3]]\n",
    "pd.DataFrame(data, columns=[\"Metrics\", \"Data1\", \"Data3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29032f-9563-4cf0-a445-d66fe2df724a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ⭐ So...what's wrong with the model?\n",
    "\n",
    "Model1 performed worse on Data3.\n",
    "\n",
    "### Let's visualize some samples from Data3 of the false negatives, true positives, false positives, and true negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1bd637-92e1-440a-9772-148ddc8e22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for plotting images\n",
    "def plot_images(images, title, n_rows=3, n_cols=3):\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i])\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d635f0-8e89-4628-b9ef-66b571560e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get tp, fp, tn, fn samples for visualization\n",
    "false_negatives = df_results[(df_results.Label == 1) & (df_results.Prediction==0)].sample(9)\n",
    "true_positives = df_results[(df_results.Label == 1) & (df_results.Prediction==1)].sample(9)\n",
    "false_positives = df_results[(df_results.Label == 0) & (df_results.Prediction==1)].sample(9)\n",
    "true_negatives = df_results[(df_results.Label == 0) & (df_results.Prediction==0)].sample(9)\n",
    "\n",
    "fn = np.vstack([load_and_preprocess_image(img_path) for img_path in false_negatives.Path])\n",
    "tp = np.vstack([load_and_preprocess_image(img_path) for img_path in true_positives.Path])\n",
    "fp = np.vstack([load_and_preprocess_image(img_path) for img_path in false_positives.Path])\n",
    "tn = np.vstack([load_and_preprocess_image(img_path) for img_path in true_negatives.Path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a178d65-0840-43dc-bde8-4b290b889ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some true positive samples\n",
    "plot_images(tp, f\"True Positive\")\n",
    "\n",
    "# Plot some false negative samples\n",
    "plot_images(fn, f\"False Negative\")\n",
    "\n",
    "# Plot some false positive samples\n",
    "plot_images(fp, f\"False Positive\")\n",
    "\n",
    "# Plot some true negative samples\n",
    "plot_images(tn, f\"True Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0a9cb-d529-47ba-9cfe-7e48f32fb637",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Any guesses? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06f827-c7bc-4256-ba95-e62688939b60",
   "metadata": {},
   "source": [
    "#### Will it help if I give you some heatmpas for visualization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645b8cd-25f6-4b21-a0c7-2b62e8d927ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = keras.models.Model(\n",
    "        model.inputs, [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_gradcam(img, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "\n",
    "    img = (img*255).astype(\"uint8\")\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = plt.colormaps[\"jet\"]\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    #display(Image(cam_path))\n",
    "\n",
    "    return cam_path\n",
    "\n",
    "def plot_cams(images, title, n_rows=3, n_cols=3):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        heatmap = make_gradcam_heatmap(images[i:i+1,:,:,:], model, last_conv_layer_name)\n",
    "        p = save_gradcam(images[i,:,:,:], heatmap)\n",
    "        tp_gradcam = plt.imread(p)\n",
    "        ax.imshow(tp_gradcam)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219ea87-466a-4e71-aa8d-728699cbf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"conv5_block3_3_conv\"\n",
    "model.layers[-1].activation = None\n",
    "heatmap = make_gradcam_heatmap(tp[0:1,:,:,:], model, last_conv_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9bd2b-d947-4713-9231-fbaebd23233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some true positive samples\n",
    "plot_cams(tp, f\"True Positive\")\n",
    "\n",
    "# Plot some false negative samples\n",
    "plot_cams(fn, f\"False Negative\")\n",
    "\n",
    "# Plot some false positive samples\n",
    "plot_cams(fp, f\"False Positive\")\n",
    "\n",
    "# Plot some true negative samples\n",
    "plot_cams(tn, f\"True Negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datathon24",
   "language": "python",
   "name": "datathon24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
